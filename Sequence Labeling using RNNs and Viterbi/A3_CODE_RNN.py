# -*- coding: utf-8 -*-
"""Assignment 3

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1VvnEZyHHWgWfMaTNVlUOEkBVlvYlzi3_

**ASSIGNMENT 3: RNN MODELS**

Sanjay Shrikanth, Matthew Daxner, Soren Larsen,
Collin McColl

<table align="left">
  <td>
    <a target="_blank" href="https://colab.research.google.com/github/jflanigan/handson-ml2/blob/master/16_nlp_with_rnns_and_attention.ipynb"><img src="https://www.tensorflow.org/images/colab_logo_32px.png" />Run in Google Colab</a>
  </td>
</table>

First, let's import a few common modules, ensure MatplotLib plots figures inline and prepare a function to save the figures. We also check that Python 3.5 or later is installed (although Python 2.x may work, it is deprecated so we strongly recommend you use Python 3 instead), as well as Scikit-Learn ≥0.20 and TensorFlow ≥2.0.
"""

# Commented out IPython magic to ensure Python compatibility.
# Python ≥3.5 is required
import sys
assert sys.version_info >= (3, 5)

# Scikit-Learn ≥0.20 is required
import sklearn
assert sklearn.__version__ >= "0.20"

try:
    # %tensorflow_version only exists in Colab.
#     %tensorflow_version 2.x
    !pip install -q -U tensorflow-addons
    IS_COLAB = True
except Exception:
    IS_COLAB = False

# TensorFlow ≥2.0 is required
import tensorflow as tf
from tensorflow import keras
assert tf.__version__ >= "2.0"

if not tf.test.is_gpu_available():
    print("No GPU was detected. LSTMs and CNNs can be very slow without a GPU.")
    if IS_COLAB:
        print("Go to Runtime > Change runtime and select a GPU hardware accelerator.")

# Common imports
import numpy as np
import os

# to make this notebook's output stable across runs
np.random.seed(42)
tf.random.set_seed(42)

# To plot pretty figures
# %matplotlib inline
import matplotlib as mpl
import matplotlib.pyplot as plt
mpl.rc('axes', labelsize=14)
mpl.rc('xtick', labelsize=12)
mpl.rc('ytick', labelsize=12)

# Where to save the figures
PROJECT_ROOT_DIR = "."
CHAPTER_ID = "nlp"
IMAGES_PATH = os.path.join(PROJECT_ROOT_DIR, "images", CHAPTER_ID)
os.makedirs(IMAGES_PATH, exist_ok=True)

def save_fig(fig_id, tight_layout=True, fig_extension="png", resolution=300):
    path = os.path.join(IMAGES_PATH, fig_id + "." + fig_extension)
    print("Saving figure", fig_id)
    if tight_layout:
        plt.tight_layout()
    plt.savefig(path, format=fig_extension, dpi=resolution)

"""# Sentiment Analysis"""

tf.random.set_seed(42)

"""You can load the IMDB dataset easily:"""

(X_train, y_test), (X_valid, y_test) = keras.datasets.imdb.load_data()

X_train[0][:10]

word_index = keras.datasets.imdb.get_word_index()
id_to_word = {id_ + 3: word for word, id_ in word_index.items()}
for id_, token in enumerate(("<pad>", "<sos>", "<unk>")):
    id_to_word[id_] = token
" ".join([id_to_word[id_] for id_ in X_train[0][:10]])

import tensorflow_datasets as tfds

datasets, info = tfds.load("imdb_reviews", as_supervised=True, with_info=True)

datasets.keys()

train_size = info.splits["train"].num_examples
test_size = info.splits["test"].num_examples

train_size, test_size

for X_batch, y_batch in datasets["train"].batch(2).take(1):
    for review, label in zip(X_batch.numpy(), y_batch.numpy()):
        print("Review:", review.decode("utf-8")[:200], "...")
        print("Label:", label, "= Positive" if label else "= Negative")
        print()

def preprocess(X_batch, y_batch):
    X_batch = tf.strings.substr(X_batch, 0, 300)
    X_batch = tf.strings.regex_replace(X_batch, rb"<br\s*/?>", b" ")
    X_batch = tf.strings.regex_replace(X_batch, b"[^a-zA-Z']", b" ")
    X_batch = tf.strings.split(X_batch)
    return X_batch.to_tensor(default_value=b"<pad>"), y_batch

preprocess(X_batch, y_batch)

from collections import Counter

vocabulary = Counter()
for X_batch, y_batch in datasets["train"].batch(32).map(preprocess):
    for review in X_batch:
        vocabulary.update(list(review.numpy()))

vocabulary.most_common()[:3]

len(vocabulary)

vocab_size = 10000
truncated_vocabulary = [
    word for word, count in vocabulary.most_common()[:vocab_size]]

word_to_id = {word: index for index, word in enumerate(truncated_vocabulary)}
for word in b"This movie was faaaaaantastic".split():
    print(word_to_id.get(word) or vocab_size)

words = tf.constant(truncated_vocabulary)
word_ids = tf.range(len(truncated_vocabulary), dtype=tf.int64)
vocab_init = tf.lookup.KeyValueTensorInitializer(words, word_ids)
num_oov_buckets = 1000
table = tf.lookup.StaticVocabularyTable(vocab_init, num_oov_buckets)

table.lookup(tf.constant([b"This movie was faaaaaantastic".split()]))

def encode_words(X_batch, y_batch):
    return table.lookup(X_batch), y_batch

train_set = datasets["train"].repeat().batch(32).map(preprocess)
train_set = train_set.map(encode_words).prefetch(1)

for X_batch, y_batch in train_set.take(1):
    print(X_batch)
    print(y_batch)

test_set = datasets["test"].repeat().batch(32).map(preprocess)
test_set = test_set.map(encode_words).prefetch(1)

### 1.1
"""
HERE IS OUR MODEL FOR 1.1 (SIMPLE RNN MODEL)
"""

from keras.models import Model, Sequential
from keras.layers import Embedding, Dense, Dropout, Input, LSTM, GRU, SimpleRNN

embedding_size = 16
def build_model():
  model = Sequential()
  model.add(Embedding(vocab_size + num_oov_buckets, 
                      embedding_size, 
                      mask_zero=True, 
                      input_shape=[None]))
  
  model.add(SimpleRNN(40, dropout=0.5, return_sequences=True))
  model.add(Dense(units=1, activation='tanh'))
  return model

simple_model = build_model()
simple_model.compile(loss='binary_crossentropy', 
              optimizer=keras.optimizers.Adam(0.01), 
              metrics=['accuracy'])

simple_model.summary()
history = simple_model.fit(train_set, 
                    steps_per_epoch=train_size // 32, 
                    batch_size=32, 
                    epochs=20,  
                    shuffle=True)

simple_model.evaluate(test_set, steps=test_size, batch_size=32)

##1.2
"""
HERE IS OUR LSTM MODEL
"""

from keras.models import Model, Sequential
from keras.layers import Embedding, Dense, Dropout, Input, LSTM, GRU, SimpleRNN

embedding_size = 16
def build_lstm():
  model = Sequential()
  model.add(Embedding(vocab_size + num_oov_buckets, 
                      embedding_size, 
                      mask_zero=True, 
                      input_shape=[None]))
  
  model.add(LSTM(40))
  model.add(Dense(units=64, activation='tanh'))
  model.add(Dropout(0.5))
  model.add(Dense(units=1, activation='tanh'))
  return model

model = build_lstm()
model.compile(loss='binary_crossentropy', 
              optimizer=keras.optimizers.Adam(0.005), 
              metrics=['accuracy'])

model.summary()
history = model.fit(train_set, 
                    steps_per_epoch=train_size // 32, 
                    batch_size=32, 
                    epochs=20,  
                    shuffle=True)

model.evaluate(test_set, steps=test_size, batch_size=32)